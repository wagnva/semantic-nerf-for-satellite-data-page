<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Semantic Neural Radiance Fields for Multi-Date Satellite Data">
  <meta name="keywords" content="Semantic, Satellite Data, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Semantic Neural Radiance Fields for Multi-Date Satellite Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- Favicon Stuff -->
  <link rel="icon" type="image/png" href="./static/images/favicon/favicon-96x96.png" sizes="96x96" />
  <link rel="icon" type="image/svg+xml" href="./static/images/favicon/favicon.svg" />
  <link rel="shortcut icon" href="./static/images/favicon/favicon.ico" />
  <link rel="apple-touch-icon" sizes="180x180" href="./static/images/favicon/apple-touch-icon.png" />
  <link rel="manifest" href="./static/images/favicon/site.webmanifest" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- https://github.com/sneas/img-comparison-slider -->
  <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> -->

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Semantic Neural Radiance Fields for <br>Multi-Date Satellite Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Valentin Wagner,</span>
            <span class="author-block">
              Sebastian Bullinger,</span>
            <span class="author-block">
              Christoph Bodensteiner,</span>
            <span class="author-block">
              and Michael Arens</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Fraunhofer Institute of Optronics, System Technologies and Image Exploitation</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this work we propose a satellite specific Neural Radiance
            Fields (NeRF) model capable to obtain a threedimensional
            semantic representation (neural semantic field)
            of the scene. The model derives the output from a set of
            multi-date satellite images with corresponding pixel-wise
            semantic labels. We demonstrate the robustness of our approach
            and its capability to improve noisy input labels. We
            enhance the color prediction by utilizing the semantic information
            to adress temporal image inconsistencies caused
            by non-stationary categories such as vehicles.
          </p>
          <p>
            To facilitate
            further research in this domain, we present a dataset comprising
            manually generated labels for popular multi-view
            satellite images.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <div class="columns">

        <div class="column">
          <div class="content">
            <video id="jax214_rgb" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/jax_214_rgb.webm"
                      type="video/mp4">
            </video>
            <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/iQ1-orKvd_I?si=9ika5E2Id3ivay9Q&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>         -->
          </div>
        </div>
  
        <div class="column">
          <div class="content">
            <video id="jax214_semantic" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/jax_214_semantic.webm"
                      type="video/mp4">
            </video>
            <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/-UcKHpQlntU?si=T7a_rdhMGgoMgLjn&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>        </div> -->
        </div>

      </div>
    </div>

    
    <p>Color and Semantic Images are rendered in the same inference call using a shared three-dimensional structure.</p>

  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="publication-video">
          <img src="static/images/Pipeline.png" alt="">
        </div>
        <div class="content has-text-justified">
          <p>
            The satellite-domain-adapted outputs (i.e. elements in the blue area) are combined using an
            irradiance lighting model to produce the color rendering as originally proposed by <a href="https://arxiv.org/abs/2203.08896">SatNeRF</a>. Using an additional semantic head (i.e.
            elements in the red area) our proposed method is able to produce a corresponding semantic pixel-wise labeling. We combine this with
            the learned lighting scalar to create a three-dimensional semantic visualization. We introduce a transient regularization loss \(L_t\) to reduce
            artifacts in the learned appearance based on the semantic input data.
          </p>
        </div>
      </div>
    </div> 
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop has-text-justified">

    <h2 class="title">Semantic Neural Radiance Fields for Multi-Date Satellite Data</h2>

    <p>
      <b>Goal:</b> Learn an implicit, combined color and semantic 3D representation of a scene based on a set of satellite images (~10-20 images) including their camera (RPC) data.
    </p>

    <p class="mt-2"><b>Our Contribution:</b> We extend the satellite-domain-adapted <a href="https://arxiv.org/abs/2203.08896">SatNeRF</a> to include semantic as additional modality besides color. Our proposed semantic satellite-domain-adapted NeRF models following function: </p>

    <p class="mt-3">
      \(\large \mathcal{F}: (\mathbf{x}, \mathbf{\omega}, \mathbf{t}_j) \mapsto (\sigma, \mathbf{c}_a, \mathit{sun}, \mathbf{a}, \beta, \mathbf{s})\)
    </p>

    <div class="columns is-centered is-mobile is-multiline mt-0">
      <div class="column is-half-mobile pt-1 pb-0">
        <p><b>Inputs: </b></p>
        <p>\(\mathbf{x} = \text{3D Position}  \)</p>
        <p>\(\mathbf{t}_j = \text{Transient Embedding} \)</p>
        <p>\(\omega = \text{Sun Direction} \)</p>
      </div>

      <div class="column is-half-mobile pt-1 pb-0">
        <p><b>Geometry: </b></p>
        <p>\(\sigma = \text{Density} \)</p>
      </div>

      <div class="column is-half-mobile pt-1 pb-0">
        <p><b>Color: </b></p>
        <p>\(\mathbf{c}_a = \text{Albedo Color} \)</p>
        <p>\(sun = \text{Lighting} \)</p>
        <p>\(\mathbf{a} = \text{Ambient Color} \)</p>
      </div>

      <div class="column is-half-mobile pt-1 pb-0">
        <p><b>Semantic: </b></p>
        <p>\(\mathbf{s} = \text{Semantic Logits} \)</p>
      </div>
    </div>

    <p class="mt-0">
      <b>Usage:</b> For each pixel a visual ray \(\mathbf{r}\) is created, sampled at positions \(\mathbf{x}_i\) and the outputs of \(\mathcal{F}\) are aggregated based on the visibility \(\alpha\) and transmittance \(T\)
    </p>

    <p class="mt-1">
      \(\large \mathbf{s}(\mathbf{r}) = \sum_{i=1}^{N} T_i \alpha_i \mathbf{s}(\mathbf{x}_i)	{\normalsize \text{ with }} 	\alpha_i = 1 - \exp(-\sigma_i\delta_i) {\normalsize \text{ and }} T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)  \)
    </p>

    <p class="mt-1">
      Combined Semantic logits \(\mathbf{s}(\mathbf{r})  \) are converted into probabilities \(\mathbf{p}(\mathbf{r}) \) using softmax. The class with the highest probability is chosen to represent the semantic class of the ray \(\mathbf{r}\).
      Both modalities (color and semantic) use a <b>unified, shared</b> geometrical representation, leading to a high cohesion.
      To learn the semantic modality, a secondary loss term is introduced. Analog to color we focus on static, non-transient semantic classes:
    </p>
    <p class="mt-1">
      \(\large L_{semantic}(\mathcal{R} \setminus \mathcal{R}^t) = - \sum_{\mathbf{r} \in (\mathcal{R} \setminus \mathcal{R}^t )} \sum_{c=1}^{C} p^c(\mathbf{r}) \log{\hat{p}^c(\mathbf{r})}
      {\normalsize \text{ with } }
      \mathcal{R}_t = \{\ \mathbf{r} \in \mathcal{R} | semantic_{gt}(\mathbf{r}) = vehicle \} \)
    </p>

    <p class="mt-5">
      <b>Results:</b> We are able to render novel, unseen views in both modalities using a single inference call.
    </p>

    <div class="columns is-centered is-mobile is-multiline mt-1">
      <div class="column is-half-mobile has-text-centered">
          <img src="./static/images/results/gt_rgb/JAX_004_015_RGB.jpg" alt="">
          <p class="is-hidden-tablet">(a) RGB Ground Truth.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/gt/JAX_004_015_CLS.jpg" alt="">
        <p class="is-hidden-tablet">(b) Our Annotations.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic/JAX_004_015_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(c) Predicted Semantic.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/shadows/JAX_004_015_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(d) Predicted Lighting.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic_shaded/JAX_004_015_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(e) Structural Semantic Visualization.</p>
      </div>
    </div>

    <div class="columns is-centered is-mobile is-multiline mt-1">
      <div class="column is-half-mobile has-text-centered">
          <img src="./static/images/results/gt_rgb/JAX_068_003_RGB.jpg" alt="">
          <p class="is-hidden-tablet">(a) RGB Ground Truth.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/gt/JAX_068_003_CLS.jpg" alt="">
        <p class="is-hidden-tablet">(b) Our Annotations.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic/JAX_068_003_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(c) Predicted Semantic.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/shadows/JAX_068_003_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(d) Predicted Lighting.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic_shaded/JAX_068_003_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(e) Structural Semantic Visualization.</p>
      </div>
    </div>

    <div class="columns is-centered is-mobile is-multiline mt-1">
      <div class="column is-half-mobile has-text-centered">
          <img src="./static/images/results/gt_rgb/JAX_214_002_RGB.jpg" alt="">
          <p class="is-hidden-tablet">(a) RGB Ground Truth.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/gt/JAX_214_002_CLS.jpg" alt="">
        <p class="is-hidden-tablet">(b) Our Annotations.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic/JAX_214_002_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(c) Predicted Semantic.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/shadows/JAX_214_002_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(d) Predicted Lighting.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic_shaded/JAX_214_002_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(e) Structural Semantic Visualization.</p>
      </div>
    </div>

    <div class="columns is-centered is-mobile is-multiline mt-1">
      <div class="column is-half-mobile has-text-centered">
          <img src="./static/images/results/gt_rgb/JAX_260_007_RGB.jpg" alt="">
          <p class="is-hidden-tablet">(a) RGB Ground Truth.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/gt/JAX_260_007_CLS.jpg" alt="">
        <p class="is-hidden-tablet">(b) Our Annotations.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic/JAX_260_007_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(c) Predicted Semantic.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/shadows/JAX_260_007_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(d) Predicted Lighting.</p>
      </div>

      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/results/semantic_shaded/JAX_260_007_RGB_-1.png" alt="">
        <p class="is-hidden-tablet">(e) Structural Semantic Visualization.</p>
      </div>
    </div>

    <div class="columns is-centered is-mobile is-multiline mt-0 is-hidden-mobile">
      <div class="column is-half-mobile has-text-centered pt-0">
          <p>(a) RGB Ground Truth.</p>
      </div>

      <div class="column is-half-mobile has-text-centered pt-0">
        <p>(b) Our Annotations.</p>
      </div>

      <div class="column is-half-mobile has-text-centered pt-0">
        <p>(c) Predicted Semantic.</p>
      </div>

      <div class="column is-half-mobile has-text-centered pt-0">
        <p>(d) Predicted Lighting.</p>
      </div>

      <div class="column is-half-mobile has-text-centered pt-0">
        <p>(e) Structural Semantic Visualization.</p>
      </div>
    </div>

    </div>
  </section>


<section class="section">
  <div class="container is-max-desktop has-text-justified">

    <h2 class="title">Transient Regularization</h2>

    <p><b>Issue:</b> Due to the dataset images being captured over a span of time (Multi-Date), each image contains a varying set of moving, transient objects such as vehicles. 
      Neural Radiance Fields reconstruct the pixel colors assuming an inherent multi-view consistency.
      This can cause local inconsistencies and artefacts in the learned color representation as the network tries to average between all given input images.
    </p>

    <p class="mt-3">
      The same parking garage across a series of training images including their capture date:
    </p>
    <div class="columns is-centered is-mobile is-multiline mt-1">
      <div class="column is-half-mobile has-text-centered">
          <img src="./static/images/dataset_transients/JAX_214_005_RGB_cropped.png" alt="">
          <!-- 20141030155732 -->
          <h5 class="subtitle is-6 mt-2">October 2014</h5> 
      </div>
      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/dataset_transients/JAX_214_008_RGB_cropped.png" alt="">
        <!-- 20150121161243 -->
        <h5 class="subtitle is-6 mt-2">January 2015</h5>
      </div>
      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/dataset_transients/JAX_214_012_RGB_cropped.png" alt="">
        <!-- 20150215161208 -->
        <h5 class="subtitle is-6 mt-2">February 2015</h5>
      </div>
      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/dataset_transients/JAX_214_015_RGB_cropped.png" alt="">
        <!-- 20150501160357 -->
        <h5 class="subtitle is-6 mt-2">May 2015</h5>
      </div>
      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/dataset_transients/JAX_214_016_RGB_cropped.png" alt="">
        <!-- 20150502161943 -->
        <h5 class="subtitle is-6 mt-2">May 2015</h5>
      </div>
    </div>

    <p>
      <b>Goal:</b> Reduce artefacts caused by moving, transient objects such as vehicles.
    </p>

    <p class="mt-5">
      Analog to <a href="https://nerf-w.github.io/">Nerf-in-the-Wild</a> we propose the use of a learned uncertainty \( \beta(\mathbf{x}, \mathbf{t}_j) \). \(\mathbf{x}\) hereby refers to a 3-dimensional position in the scene 
      and \(\mathbf{t}_j\) is a learned embedding for each training image \(j\). 
      The color loss \(L_{color}\) is modified to include the uncertainty, allowing the network to reduce the impact of pixels belonging to transient objects by increasing the uncertainty \(\beta \).
    </p>

    <p class="mt-3">
      \( \large L_{color}(\mathcal{R}) = \sum_{\mathbf{r} \in \mathcal{R}} \frac{|| \mathbf{c}(\mathbf{r}) - \mathbf{c}_{GT}(\mathbf{r}) ||^2_2}{2\beta'(\mathbf{r})^2} + \left( \frac{\log{\beta'(\mathbf{r}) + \eta}}{2} \right) \)
    </p>

    <p class="mt-5">
      <b>Our Contribution:</b> We propose a transient regularization loss \( L_t \), using existing semantic data to increase uncertainty \(\beta \) for known transient locations based on existing ground truth semantic data.
    </p>

    <p class="mt-3">
      \( \large L_{t}(\mathcal{R}^t) = \sum_{r \in \mathcal{R}^t} || 1.0 - \beta(r)  ||^2_2 
      {\normalsize \text{ with } }
      \mathcal{R}_t = \{\ \mathbf{r} \in \mathcal{R} | semantic_{gt}(\mathbf{r}) = vehicle \} \)
    </p>

    <p class="mt-5">
      <b>Results:</b> We show a strong decrease in artefacts, mainly in parking lots and on top of parking garages.
    </p>

    <div class="columns is-vcentered mt-3">

        <!-- <div class="column">
          <div class="image-comp">
            <div style="background-image: url('./static/images/dataset/JAX_004_012_RGB.jpg')" alt="" class="img background"> </div>
            <div style="background-image: url('./static/images/dataset/JAX_004_012_CLS.jpg')" alt="" class="img foreground"> </div>
            <input type="range" min="1" max="100" value="50" name='slider' class="slider">
            <div class='slider-button'></div>
          </div>
        </div> -->

        <!-- Before/After for JAX068 -->
        <div class="column">
          <img-comparison-slider class="slider-with-animated-handle">
            <figure slot="first" class="before">
              <img width="100%" src="./static/images/car_reg/JAX_068_010_RGB_-1_NCR.png">
              <figcaption>SatNeRF</figcaption>
            </figure>

            <figure slot="second" class="after">
              <img width="100%" src="./static/images/car_reg/JAX_068_010_RGB_-1_WCR.png">
              <figcaption>Ours</figcaption>
            </figure>

            <svg slot="handle" class="custom-animated-handle" xmlns="http://www.w3.org/2000/svg" width="100" viewBox="-8 -3 16 6">
              <path stroke="#fff" d="M -5 -2 L -7 0 L -5 2 M -5 -2 L -5 2 M 5 -2 L 7 0 L 5 2 M 5 -2 L 5 2" stroke-width="1" fill="#fff" vector-effect="non-scaling-stroke"></path>
            </svg>
          </img-comparison-slider>
        </div>

        <!-- Before/After for JAX214 -->
        <div class="column">
          <img-comparison-slider class="slider-with-animated-handle">
            <figure slot="first" class="before">
              <img width="100%" src="./static/images/car_reg/JAX_214_019_RGB_-1_NCR.png">
              <figcaption>SatNeRF</figcaption>
            </figure>

            <figure slot="second" class="after">
              <img width="100%" src="./static/images/car_reg/JAX_214_019_RGB_-1_WCR.png">
              <figcaption>Ours</figcaption>
            </figure>

            <svg slot="handle" class="custom-animated-handle" xmlns="http://www.w3.org/2000/svg" width="100" viewBox="-8 -3 16 6">
              <path stroke="#fff" d="M -5 -2 L -7 0 L -5 2 M -5 -2 L -5 2 M 5 -2 L 7 0 L 5 2 M 5 -2 L 5 2" stroke-width="1" fill="#fff" vector-effect="non-scaling-stroke"></path>
            </svg>
          </img-comparison-slider>
        </div>

    </div>

    <div class="columns has-text-centered  is-hidden-mobile">
      <div class="column">
        <h5 class="subtitle is-6">JAX 068</h5>
      </div>
      <div class="column">
        <h5 class="subtitle is-6">JAX 214</h5>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title">Dataset</h2>

    <p>
      We release a dataset comprising of high-quality semantic annotations for the main Area-of-Interest for four scenes from the <a href="https://ieee-dataport.org/open-access/data-fusion-contest-2019-dfc2019">DFC-2019 Track-3 Dataset</a>.
      To create the annotations we manually refine initial rough estimates of a user guided, class agnostic foundation model for semantic segmentation.
    </p>
    <p class="mt-2">
      The dataset covers the five semantic classes: <span class="is-italic">Ground, Water, Vegetation, Building</span> and <span class="is-italic">Vehicles</span>.
    </p>

    <!-- Dataset Link. -->
    <div class="mt-3">
      <span class="link-block">
        <a href="https://github.com/google/nerfies/releases/tag/0.1"
            class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
              <i class="far fa-images"></i>
          </span>
          <span>Data</span>
          </a>
      </span>
    </div>
    

    <div class="columns is-centered is-mobile is-multiline mt-3"> 
      
      <div class="column is-half-mobile has-text-centered">
          <img src="./static/images/dataset/JAX_004_012_RGB.jpg" alt="">
          <img src="./static/images/dataset/JAX_004_012_CLS.jpg" alt="" class="mt-3">
          <h5 class="subtitle is-6 mt-2">JAX 004</h5>
      </div>
      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/dataset/JAX_068_012_RGB.jpg" alt="">
        <img src="./static/images/dataset/JAX_068_012_CLS.jpg" alt="" class="mt-3">
        <h5 class="subtitle is-6 mt-2">JAX 068</h5>
      </div>
      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/dataset/JAX_214_012_RGB.jpg" alt="">
        <img src="./static/images/dataset/JAX_214_012_CLS.jpg" alt="" class="mt-3">
        <h5 class="subtitle is-6 mt-2">JAX 214</h5>
      </div>
      <div class="column is-half-mobile has-text-centered">
        <img src="./static/images/dataset/JAX_260_012_RGB.jpg" alt="">
        <img src="./static/images/dataset/JAX_260_012_CLS.jpg" alt="" class="mt-3">
        <h5 class="subtitle is-6 mt-2">JAX 260</h5>
      </div>

    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>BibTeX Code Missing</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website is made using the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> template.
            <br>
            Favicon icon used from <a href="https://www.flaticon.com/free-icons/globe" title="Flaticon.com">Freepik - Flaticon</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
